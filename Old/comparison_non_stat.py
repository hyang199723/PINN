# Simulate a non-stationary process #%% Packagesimport torchimport numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npimport randomfrom sklearn.metrics import pairwise_distancesfrom sklearn.model_selection import train_test_splitimport torch.nn as nnimport torch.optim as optimfrom collections import OrderedDictif torch.cuda.is_available():    device = torch.device('cuda')else:    device = torch.device('cpu')    #%% Simulate 2-D Non-stationary process# The mean will be 0 with exponential correlation# N: total number of simulations# rho: spatial correlation# vvv: variance# return: X: predictors# Y: response# Generate more non-stationary process# Cut the domain in half, one region use smoothness of 0.5, # another region of 2 or 3def simulate(N, rho, vvv):    n = N    random.seed(123)    length = 20    coords_x = np.random.uniform(0, length, n)    random.seed(123)    coords_y = np.random.uniform(0, length, n)    coords = np.zeros((n, 2))    coords[:, 0] = coords_x    coords[:, 1] = coords_y**2    X = np.zeros((n, 3))    X[:, 0] = 1    X[:, 1] = coords_x    X[:, 2] = coords_y    # Exponential Correlation    distance = pairwise_distances(coords.reshape(-1, 2))    corr = np.exp(-distance / rho)    # Cholesky decomposition and generate correlated data    L = np.linalg.cholesky(vvv*corr)    random.seed(123)    z = np.random.normal(0, 1, n)    Y = np.dot(L, z)    #plt.scatter(x = coords, y = Y)    return X, YN = 1000v = 1spatial_corr = 3X, Y = simulate(N, rho = spatial_corr, vvv = v)plt.scatter(X[:,1], X[:,2], s = 20, c = Y)plt.colorbar()#%% Split the data setdef random_split(X, Y):    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)    return X_train, X_test, y_train, y_testX_train, X_test, y_train, y_test = random_split(X, Y)plt.scatter(X_train[:,1], X_train[:,2], s = 20, c = y_train)plt.colorbar()plt.title("Training set")plt.scatter(X_test[:,1], X_test[:,2], s = 20, c = y_test)plt.colorbar()plt.title("Testing set")#%% Krigingdef Kriging(X_train, X_test, y_train, y_test, N, v, spatial_corr):    n1 = X_train.shape[0] # Train size    n2 = X_test.shape[0] # Test size    s_train = X_train[:, 1:3]    s_test = X_test[:, 1:3]    coords = np.concatenate((s_test, s_train))    distance = pairwise_distances(coords.reshape(-1, 2))    v_bar = np.var(y_train)    cov_bar = v_bar * np.exp(-distance / spatial_corr)    sigma11 = cov_bar[0:n2, 0:n2]    sigma12 = cov_bar[0:n2, n2:N]    sigma21 = np.transpose(sigma12)    sigma22 = cov_bar[n2:N, n2:N]    sigma22_inv = np.linalg.inv(sigma22)    sigma_bar = sigma11 - np.dot(np.dot(sigma12, sigma22_inv), sigma21)    mu_bar = np.dot(np.dot(sigma12, sigma22_inv), y_train)    zzz = np.random.normal(0, 1, n2)    L = np.linalg.cholesky(sigma_bar)    y_hat = mu_bar + np.dot(L, zzz)        plt.subplot(1, 2, 1)    plt.scatter(X_test[:,1], X_test[:,2], s = 20, c = y_hat)    plt.colorbar()    plt.title("Kriging result")    plt.subplot(1, 2, 2)    plt.scatter(X_test[:,1], X_test[:,2], s = 20, c = y_test)    plt.colorbar()    plt.title("Testing set")    plt.show()    # Get MSE    mse_krig = np.sum((y_hat - y_test)**2 ) / n2        return mse_krig#%% Neural Network feed forward without penaltyclass FeedForwardNN(nn.Module):    def __init__(self, layers):        super(FeedForwardNN, self).__init__()                # parameters        self.depth = len(layers) - 1                # set up layer order dict        self.activation = torch.nn.Tanh                layer_list = list()        for i in range(self.depth - 1):             layer_list.append(                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))            )            layer_list.append(('activation_%d' % i, self.activation()))                    layer_list.append(            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))        )        layerDict = OrderedDict(layer_list)                # deploy layers        self.layers = torch.nn.Sequential(layerDict)            def forward(self, x):        out = self.layers(x)        return outdef model_train_dnn(X_train, y_train, epochs):    # Train-test split    y_train = y_train.reshape(-1, 1)    # Initialize model, loss, and optimizer    y_train_tc = torch.from_numpy(y_train).to(torch.float32)    criterion = nn.MSELoss()    X_train_tc = torch.tensor(X_train).float().to(device)            layers = [3, 10, 10, 10, 1]    model = FeedForwardNN(layers)    criterion = nn.MSELoss()    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)    model.train()    for epoch in range(epochs):        y_pred = model(X_train_tc)                loss = criterion(y_pred, y_train_tc)                optimizer.zero_grad()                loss.backward()                optimizer.step()        #print(loss)    return modeldef nn_train(X_train, X_test, y_train, y_test, epos):    n1 = X_train.shape[0] # Train size    n2 = X_test.shape[0] # Test size    model = model_train_dnn(X_train, y_train, epochs=epos)    X_test_tc = torch.from_numpy(X_test).float().to(device)    y_pred = model(X_test_tc)    y_hat = y_pred.detach().numpy().reshape(-1,)    # Get MSE    mse_nn = np.sum((y_hat - y_test)**2 ) / n2    return mse_nn#%% Many itersiters = 50kriging_mse = 0nn_mse = 0N = 1000v = 1spatial_corr = 3for i in range(iters):    X, Y = simulate(N, rho = spatial_corr, vvv = v)    X_train, X_test, y_train, y_test = random_split(X, Y)    kriging_mse += Kriging(X_train, X_test, y_train, y_test, N, v, spatial_corr)    nn_mse += nn_train(X_train, X_test, y_train, y_test, 16000)    kriging_mse /= iters # 0.9287nn_mse /= iters  # 0.8797# Y_train var 0.96v = 10N = 1000X, Y = simulate(N, rho = spatial_corr, vvv = v)X_train, X_test, y_train, y_test = random_split(X, Y)Kriging(X_train, X_test, y_train, y_test, N, v, spatial_corr)#%% Plotting # Plot